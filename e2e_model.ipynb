{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01847cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoConfig, AutoModel, BertConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from src.data import parse_file, collate_fn\n",
    "import tqdm\n",
    "import json\n",
    "from transformers.optimization import AdamW\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from src.losses import ATLoss\n",
    "from src.util import process_long_input\n",
    "from transformers import BertConfig, RobertaConfig, DistilBertConfig, XLMRobertaConfig\n",
    "from itertools import groupby\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a2e1bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Parsing & generating candidates (n=9): 100%|████| 35/35 [00:00<00:00, 90.14it/s]\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "## Model Configuration ##\n",
    "#########################\n",
    "language_model = 'bert-base-uncased'\n",
    "lm_config = AutoConfig.from_pretrained(\n",
    "    language_model,\n",
    "    num_labels=10,\n",
    ")\n",
    "lm_model = AutoModel.from_pretrained(\n",
    "    language_model,\n",
    "    from_tf=False,\n",
    "    config=lm_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "with open(\"data/Ontology/roles.json\") as f:\n",
    "    relation_types = json.load(f)\n",
    "with open(\"data/Ontology/trigger_entity_types.json\") as f:\n",
    "    mention_types = json.load(f)\n",
    "with open(\"data/Ontology/feasible_roles.json\") as f:\n",
    "    feasible_roles = json.load(f)\n",
    "\n",
    "max_n = 9\n",
    "train_loader = DataLoader(\n",
    "    parse_file(\"data/WikiEvents/DocRed_Format/train_small.json\",\n",
    "    tokenizer=tokenizer,\n",
    "    relation_types=relation_types,\n",
    "    max_candidate_length=max_n),\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7d8900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config, model, cls_token_id, sep_token_id, relation_types, mention_types, feasible_roles, soft_mention = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.model = model\n",
    "\n",
    "        self.entity_anchor = nn.Parameter(torch.zeros((66, 768)))\n",
    "        torch.nn.init.uniform_(self.entity_anchor, a=-1.0, b=1.0)\n",
    "        \n",
    "        self.relation_embeddings = nn.Parameter(torch.zeros((57,3*768)))\n",
    "        torch.nn.init.uniform_(self.relation_embeddings, a=-1.0, b=1.0)            \n",
    "        self.nota_embeddings = nn.Parameter(torch.zeros((20,3*768)))\n",
    "        torch.nn.init.uniform_(self.nota_embeddings, a=-1.0, b=1.0)\n",
    "\n",
    "\n",
    "        self.triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "        self.at_loss = ATLoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.soft_mention = soft_mention\n",
    "        self.k_mentions = 50\n",
    "                \n",
    "        self.cls_token_id = cls_token_id\n",
    "        self.sep_token_id = sep_token_id\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "        self.relation_types = relation_types\n",
    "        self.mention_types = mention_types\n",
    "        self.feasible_roles = feasible_roles\n",
    "        \n",
    "        \n",
    "    def encode(self, input_ids, attention_mask):\n",
    "        config = self.config\n",
    "        if type(config) == BertConfig or type(config) == DistilBertConfig:\n",
    "            start_tokens = [self.cls_token_id]\n",
    "            end_tokens = [self.sep_token_id]\n",
    "        elif type(config) == RobertaConfig or type(config) == XLMRobertaConfig:\n",
    "            start_tokens = [self.cls_token_id]\n",
    "            end_tokens = [self.sep_token_id, self.sep_token_id]\n",
    "        sequence_output, attention = process_long_input(self.model, input_ids, attention_mask, start_tokens, end_tokens)\n",
    "        return sequence_output, attention\n",
    "\n",
    "\n",
    "   \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, candidate_spans, relation_labels, entity_spans, entity_types, entity_ids, batch_text):\n",
    "        sequence_output, attention = self.encode(input_ids, attention_mask)\n",
    "        loss = torch.zeros((1)).to(sequence_output)\n",
    "        mention_loss = torch.zeros((1)).to(sequence_output)\n",
    "        counter = 0\n",
    "        batch_triples = []\n",
    "        batch_events = []\n",
    "        \n",
    "        if not self.training:\n",
    "            entity_spans = [[] for _ in range(sequence_output.shape[0])]\n",
    "            entity_spans = [[] for _ in range(sequence_output.shape[0])]\n",
    "            entity_types = [[] for _ in range(sequence_output.shape[0])]\n",
    "            entity_ids = [[] for _ in range(sequence_output.shape[0])]\n",
    "            relation_labels = [[] for _ in range(sequence_output.shape[0])]\n",
    "                    \n",
    "        for batch_i in range(sequence_output.size(0)):\n",
    "\n",
    "            # MENTION DETECTION\n",
    "\n",
    "            # ---------- Candidate span embeddings ------------\n",
    "            mention_candidates = []\n",
    "            candidates_attentions = []\n",
    "            for span in candidate_spans[batch_i]:\n",
    "                mention_embedding = torch.mean(sequence_output[batch_i, span[0]:span[1]+1,:], 0)\n",
    "                mention_attention = torch.mean(attention[batch_i, span[0]:span[1]+1,:], 0)\n",
    "                mention_candidates.append(mention_embedding)\n",
    "                candidates_attentions.append(mention_attention)\n",
    "            embs = torch.stack(mention_candidates)\n",
    "            atts = torch.stack(candidates_attentions)\n",
    "\n",
    "            # ---------- mention detection (scores) ------------\n",
    "            span_scores = embs.unsqueeze(1) * self.entity_anchor.unsqueeze(0)\n",
    "            span_scores = torch.sum(span_scores, dim=-1)\n",
    "            span_scores_max, class_for_span = torch.max(span_scores, dim=-1)\n",
    "            scores_for_max, max_spans = torch.topk(span_scores_max.view(-1), min(self.k_mentions, embs.size(0)), dim=0)\n",
    "            class_for_max_span = class_for_span[max_spans]\n",
    "\n",
    "            if self.training:\n",
    "                # ---------- Mention Loss and adding true spans during training ------------\n",
    "\n",
    "                if self.soft_mention:\n",
    "                    spans_for_type = {}\n",
    "\n",
    "                    for span, rtype in zip(entity_spans[batch_i], entity_types[batch_i]):\n",
    "                        if rtype not in spans_for_type.keys():\n",
    "                            spans_for_type[rtype] = []\n",
    "                        spans_for_type[rtype].append(span[0])\n",
    "\n",
    "                    anchors, positives, negatives = [], [], []\n",
    "\n",
    "                    for rtype, positive_examples in spans_for_type.items():\n",
    "\n",
    "                        # add negative examples from entity spans\n",
    "                        for pos in positive_examples:\n",
    "                            for rtype2, negative_examples in spans_for_type.items():\n",
    "                                if rtype2 == rtype:\n",
    "                                    continue\n",
    "                                for neg in negative_examples:\n",
    "                                    anchors.append(self.entity_anchor[self.mention_types.index(rtype),:])\n",
    "                                    positives.append(torch.mean(sequence_output[batch_i, pos[0]:pos[1]+1,:], 0))\n",
    "                                    negatives.append(torch.mean(sequence_output[batch_i, neg[0]:neg[1]+1,:], 0))\n",
    "\n",
    "                        # add negative examples from candidate spans\n",
    "                        for pos in positive_examples:\n",
    "                            for neg in [x for x in candidate_spans[batch_i] if x not in entity_spans[batch_i]]:\n",
    "                                anchors.append(self.entity_anchor[self.mention_types.index(rtype),:])\n",
    "                                positives.append(torch.mean(sequence_output[batch_i, pos[0]:pos[1]+1,:], 0))\n",
    "                                negatives.append(torch.mean(sequence_output[batch_i, neg[0]:neg[1]+1,:], 0))\n",
    "\n",
    "\n",
    "                    mention_loss += self.triplet_loss(torch.stack(anchors), torch.stack(positives), torch.stack(negatives))\n",
    "\n",
    "                else:\n",
    "                    #one-hot encoding for annotated mention labels \n",
    "                    mention_labels = torch.zeros(len(candidate_spans[batch_i]),len(self.mention_types))\n",
    "                    for idx,c in enumerate(candidate_spans[batch_i]):\n",
    "                        for ent,t in zip(entity_spans[batch_i],entity_types[batch_i]):\n",
    "                            if [c] == ent:\n",
    "                                mention_labels[idx][self.mention_types.index(t)] = 1\n",
    "                    mention_loss += self.ce_loss(mention_labels, span_scores)\n",
    "\n",
    "\n",
    "\n",
    "            # ARGUMENT ROLE LABELING\n",
    "            \n",
    "            if self.training:\n",
    "                # ---------- Pooling Entity Embeddings and Attentions ------------\n",
    "                entity_embeddings = []\n",
    "                entity_attentions = []\n",
    "                for ent in entity_spans[batch_i]:\n",
    "                    ent_embedding = torch.mean(sequence_output[batch_i, ent[0][0]:ent[0][1],:],0)\n",
    "                    entity_embeddings.append(ent_embedding)\n",
    "                    ent_attention = torch.mean(attention[batch_i,:,ent[0][0]:ent[0][1],:],1)\n",
    "                    entity_attentions.append(ent_attention)\n",
    "                if(len(entity_embeddings) == 0):\n",
    "                    continue\n",
    "                entity_embeddings = torch.stack(entity_embeddings)\n",
    "                entity_attentions = torch.stack(entity_attentions)\n",
    "            else:\n",
    "                entity_embeddings = embs[max_spans]\n",
    "                entity_attentions = atts[max_spans]\n",
    "\n",
    "                for c in class_for_max_span:\n",
    "                    entity_types[batch_i].append(self.mention_types[c])\n",
    "                for s in max_spans:\n",
    "                    entity_spans[batch_i].append([candidate_spans[batch_i][s]])\n",
    "\n",
    "                \n",
    "            # ---------- Localized Context Pooling ------------\n",
    "            relation_candidates = []\n",
    "            localized_context = []\n",
    "            concat_embs = []\n",
    "            triggers = []\n",
    "            for s in range(entity_embeddings.shape[0]):\n",
    "                if entity_types[batch_i][s].split(\".\")[-1] != \"TRIGGER\":\n",
    "                    continue\n",
    "                triggers.append(s)\n",
    "                for o in range(entity_embeddings.shape[0]):\n",
    "                    if s != o:\n",
    "\n",
    "                        relation_candidates.append((s,o))\n",
    "\n",
    "                        A_s = entity_attentions[s,:,:]\n",
    "                        A_o = entity_attentions[o,:,:]\n",
    "                        A = torch.mul(A_o,A_s)\n",
    "                        q = torch.sum(A,0)\n",
    "                        a = q / q.sum()\n",
    "                        H_T = sequence_output[batch_i].T\n",
    "                        c = torch.matmul(H_T,a)\n",
    "                        localized_context.append(c)\n",
    "\n",
    "                        concat_emb = torch.cat((entity_embeddings[s],entity_embeddings[o],c),0)\n",
    "                        concat_embs.append(concat_emb)\n",
    "            if(len(localized_context) == 0):\n",
    "                continue\n",
    "            localized_context = torch.stack(localized_context)\n",
    "            embs = torch.stack(concat_embs)\n",
    "            \n",
    "            triggers = list(set(triggers))\n",
    "            # ---------- Pairwise Comparisons and Predictions ------------\n",
    "\n",
    "            scores = torch.matmul(embs,self.relation_embeddings.T)\n",
    "            nota_scores = torch.matmul(embs,self.nota_embeddings.T)\n",
    "            nota_scores = nota_scores.max(dim=-1,keepdim=True)[0]\n",
    "            scores = torch.cat((nota_scores, scores), dim=-1)\n",
    "            predictions = torch.argmax(scores, dim=-1, keepdim=False)\n",
    "            #Achtung: NOTA wird an 0. Stelle gesetzt\n",
    "            \n",
    "            if self.training:\n",
    "            # ---------- ATLoss with one-hot encoding for true labels ------------\n",
    "                targets = []\n",
    "                for r in relation_candidates:\n",
    "                    onehot = torch.zeros(len(relation_types))\n",
    "                    if r in relation_labels[batch_i]:\n",
    "                        onehot[relation_labels[batch_i][r]] = 1.0\n",
    "                    targets.append(onehot)\n",
    "                targets = torch.stack(targets).to(self.model.device)\n",
    "                loss += self.at_loss(scores,targets)\n",
    "                counter += 1\n",
    "            \n",
    "            # ---------- Inference ------------\n",
    "            triples = []\n",
    "            for idx,pair in enumerate(relation_candidates):\n",
    "                triple = {\n",
    "                    pair:relation_types[predictions[idx]]\n",
    "                }\n",
    "                triples.append(triple)\n",
    "            batch_triples.append(triples)\n",
    "                \n",
    "            events = []\n",
    "            for t,v in groupby(triples,key=lambda x:next(iter(x.keys()))[0]):\n",
    "                t_word = entity_types[batch_i][t]\n",
    "                t_start = entity_spans[batch_i][t][0][0]\n",
    "                t_end = entity_spans[batch_i][t][0][1]\n",
    "                event_type = t_word.split(\".TRIGGER\")[0]\n",
    "                event_id = 'unk'\n",
    "\n",
    "                arguments = []\n",
    "                for d in v:\n",
    "                    dic = next(iter(d.items()))\n",
    "                    o = dic[0][1]\n",
    "                    r = dic[1]\n",
    "\n",
    "                    if r in mymodel.feasible_roles[event_type]:\n",
    "                        a_start = entity_spans[batch_i][o][0][0]\n",
    "                        a_end = entity_spans[batch_i][o][0][1]\n",
    "                        argument = {\n",
    "                            'entity_id':'unk',\n",
    "                            'role':r,\n",
    "                            'text': \" \".join(batch_text[batch_i][a_start:a_end]),\n",
    "                            'start':a_start,\n",
    "                            'end':a_end,\n",
    "                        }\n",
    "                        arguments.append(argument)\n",
    "                event = {\n",
    "                    'id': 'unk',\n",
    "                    'event_type':event_type,\n",
    "                    'trigger': {'start':t_start ,'end':t_end, 'text':\" \".join(batch_text[batch_i][t_start:t_end])},\n",
    "                    'arguments':arguments\n",
    "                }\n",
    "                events.append(event)\n",
    "            batch_events.append(events)\n",
    "        if(counter == 0):\n",
    "                return torch.autograd.Variable(loss,requires_grad=True), batch_triples, batch_events\n",
    "        else:\n",
    "            return (mention_loss+loss)/counter, batch_triples, batch_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17137054",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = Encoder(lm_config,\n",
    "                lm_model,\n",
    "                cls_token_id=tokenizer.convert_tokens_to_ids(tokenizer.cls_token), \n",
    "                sep_token_id=tokenizer.convert_tokens_to_ids(tokenizer.sep_token),\n",
    "                relation_types=relation_types,\n",
    "                mention_types=mention_types,\n",
    "                feasible_roles=feasible_roles,\n",
    "                soft_mention=False\n",
    "                )\n",
    "optimizer = AdamW(mymodel.parameters(), lr=1e-5, eps=1e-6)\n",
    "mymodel.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a22c9b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 18/18 [00:36<00:00,  2.03s/it, L=0.00]\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "event_list = []\n",
    "entity_spans_list = []\n",
    "doc_id_list = []\n",
    "text_list = []\n",
    "token_maps = []\n",
    "with tqdm.tqdm(train_loader) as progress_bar:\n",
    "    for sample in progress_bar:\n",
    "\n",
    "        token_ids, input_mask, entity_spans, entity_types, entity_ids, relation_labels, text, token_map, candidate_spans, doc_ids = sample\n",
    "\n",
    "        loss ,triples, events = mymodel(token_ids.to(device), input_mask.to(device), candidate_spans, relation_labels, entity_spans, entity_types, entity_ids, text)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        progress_bar.set_postfix({\"L\":f\"{loss.item():.2f}\"})\n",
    "        \n",
    "        for batch_i in range(token_ids.shape[0]):\n",
    "            doc_id_list.append(doc_ids[batch_i])\n",
    "            event_list.append(events[batch_i])\n",
    "            entity_spans_list.append(entity_spans[batch_i])\n",
    "            text_list.append(text[batch_i])\n",
    "            token_maps.append(token_map[batch_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a75c11",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3497df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('data/Dump/train_eval.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fd84ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred_events'] = pd.Series(event_list,index=doc_id_list)\n",
    "df['pred_events'] = df['pred_events'].fillna(\"\").apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5871557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_div(num, denom):\n",
    "    if denom > 0:\n",
    "        return num / denom\n",
    "    else:\n",
    "        return 0\n",
    "def compute_f1(predicted, gold, matched):\n",
    "    precision = safe_div(matched, predicted)\n",
    "    recall = safe_div(matched, gold)\n",
    "    f1 = safe_div(2 * precision * recall, precision + recall)\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ab4b2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_pred, idf_gold, idf_h_matched, idf_c_matched = 0,0,0,0\n",
    "clf_pred, clf_gold, clf_h_matched, clf_c_matched = 0,0,0,0\n",
    "\n",
    "for idx, row in df.iterrows(): \n",
    "    events = row['pred_events']\n",
    "    gold_events = row['event_mentions']\n",
    "\n",
    "    for ge,e in zip(gold_events,events):\n",
    "        for g_arg in ge['arguments']:\n",
    "            for arg in e['arguments']:\n",
    "                #----- Head Matches -----\n",
    "                if g_arg['start'] == arg['start'] and g_arg['end'] == arg['end']:\n",
    "                    idf_h_matched += 1\n",
    "                    idf_c_matched += 1\n",
    "                    if g_arg['role'] == arg['role']:\n",
    "                        clf_h_matched += 1\n",
    "                        clf_c_matched += 1\n",
    "                    \n",
    "                #----- Coref Matches -----\n",
    "                for coref in g_arg['corefs']:\n",
    "                    if coref['start'] == arg['start'] and coref['end'] == arg['end']:\n",
    "                        idf_c_matched += 1\n",
    "                        if g_arg['role'] == arg['role']:\n",
    "                            clf_c_matched += 1\n",
    "            idf_gold += 1\n",
    "        for arg in e['arguments']:\n",
    "            idf_pred += 1\n",
    "        clf_pred, clf_gold = idf_pred, idf_gold\n",
    "\n",
    "        #----- Identification P,R,F1 -----\n",
    "        idf_h_p, idf_h_r, idf_h_f1 = compute_f1(idf_pred, idf_gold, idf_h_matched)\n",
    "        idf_c_p, idf_c_r, idf_c_f1 = compute_f1(idf_pred, idf_gold, idf_c_matched)\n",
    "\n",
    "        #----- Classification P,R,F1 -----\n",
    "        clf_h_p, clf_h_r, clf_h_f1 = compute_f1(idf_pred, idf_gold, clf_h_matched)\n",
    "        clf_c_p, clf_c_r, clf_c_f1 = compute_f1(idf_pred, idf_gold, clf_c_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "87a79138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Identification Report *****\n",
      "*Head* Matches: 0 Precision: 0.00 Recall: 0.00 F1-Score: 0.00\n",
      "*Coref* Matches: 0 Precision: 0.00 Recall: 0.00 F1-Score: 0.00\n",
      "\n",
      "***** Classification Report *****\n",
      "*Head* Matches: 0 Precision: 0.00 Recall: 0.00 F1-Score: 0.00\n",
      "*Coref* Matches: 0 Precision: 0.00 Recall: 0.00 F1-Score: 0.00\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Identification Report *****\")\n",
    "print(f\"*Head* Matches: {idf_h_matched} Precision: {idf_h_p:.2f} Recall: {idf_h_r:.2f} F1-Score: {idf_h_f1:.2f}\")\n",
    "print(f\"*Coref* Matches: {idf_c_matched} Precision: {idf_c_p:.2f} Recall: {idf_c_r:.2f} F1-Score: {idf_c_f1:.2f}\")\n",
    "print()\n",
    "print(\"***** Classification Report *****\")\n",
    "print(f\"*Head* Matches: {clf_h_matched} Precision: {clf_h_p:.2f} Recall: {clf_h_r:.2f} F1-Score: {clf_h_f1:.2f}\")\n",
    "print(f\"*Coref* Matches: {clf_c_matched} Precision: {clf_c_p:.2f} Recall: {clf_c_r:.2f} F1-Score: {clf_c_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ab62ac",
   "metadata": {},
   "source": [
    "## Sanity Check \n",
    "**doc_id = K0C03N4OM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2eeb4294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['custody']\n",
      "['Su', '##spect']\n",
      "['evacuation']\n",
      "['Pine', 'View', 'High']\n"
     ]
    }
   ],
   "source": [
    "for ent in entity_spans_list[0]:\n",
    "    print(text_list[0][ent[0][0]:ent[0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "00f7920e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Su',\n",
       " '##spect',\n",
       " 'in',\n",
       " 'custody',\n",
       " 'for',\n",
       " 'smoking',\n",
       " 'backpack',\n",
       " 'that',\n",
       " 'forced',\n",
       " 'evacuation',\n",
       " 'of',\n",
       " 'Pine',\n",
       " 'View',\n",
       " 'High',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5c67f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 4], [11, 14], [9, 10], [0, 1]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ents = [[2,3],[10,13],[8,9],[0,1]]\n",
    "for ent in ents:\n",
    "    ent[0] = token_maps[0][ent[0]]\n",
    "    ent[1] = token_maps[0][ent[1]]\n",
    "ents\n",
    "for ent in ents:\n",
    "    print(text_list[0][ent[0]:ent[1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
