{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c176e2c",
   "metadata": {},
   "source": [
    "## Änderungen\n",
    "- Um Eventtypes zu erkennen gibt es jetzt nicht mehr 17 entities +1 trigger sondern 17 + 49 = 66 entity_types bei der mention detection -> prototype_embeddings als Variablen im Modell angepasst. Liste wird von trigger_entity_types.json geladen\n",
    "- Ausgabe von events und triples\n",
    "- Bei Events werden unten feasible roles gecheckt und nur argument roles ausgegeben, die für das event Sinn ergeben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8746383a",
   "metadata": {},
   "source": [
    "## Probleme\n",
    "- batch_text ist Rückumwandlung von tokens in text -> spans passen zu text durch subword und [CLS] offsets. Problem: Die start und end spans meiner ausgegebenen Events passen nicht zu den im WikiEvents Datensatz deswegen. Nur das 1. Subword wird jeweils als text ausgegeben. Siehe Ende des Notebooks    \n",
    "  \n",
    "  \n",
    "  \n",
    "- Bei Inferenz habe ich keine entity_types, entity_spans, relation_labels. Wie rufe ich dann trotzdem meine model.forward() Methode auf? Passt es an der markierten Stelle unten nach der mention detection zu versuchen die selbe Ausgangsstruktur wie durch Vorliegen der entity_types, entity_spans zu schaffen um wie gehabt weiter zu machen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ddef01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexrothmaier/Documents/docred_dataloader_alex/.venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoConfig, AutoModel, BertConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from src.data import parse_file, collate_fn\n",
    "import tqdm\n",
    "import json\n",
    "from transformers.optimization import AdamW\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from src.losses import ATLoss\n",
    "from src.util import process_long_input\n",
    "from transformers import BertConfig, RobertaConfig, DistilBertConfig, XLMRobertaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2580f7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Parsing & generating candidates (n=9): 100%|██| 206/206 [00:07<00:00, 26.55it/s]\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "## Model Configuration ##\n",
    "#########################\n",
    "language_model = 'bert-base-uncased'\n",
    "lm_config = AutoConfig.from_pretrained(\n",
    "    language_model,\n",
    "    num_labels=10,\n",
    ")\n",
    "lm_model = AutoModel.from_pretrained(\n",
    "    language_model,\n",
    "    from_tf=False,\n",
    "    config=lm_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "with open(\"roles.json\") as f:\n",
    "    relation_types = json.load(f)\n",
    "with open(\"trigger_entity_types.json\") as f:\n",
    "    trigger_entity_types = json.load(f)\n",
    "\n",
    "max_n = 9\n",
    "dev_loader = DataLoader(\n",
    "    parse_file(\"data/WikiEvents/train_docred_format.json\",\n",
    "    tokenizer=tokenizer,\n",
    "    relation_types=relation_types,\n",
    "    max_candidate_length=max_n),\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e37eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config, model, cls_token_id, sep_token_id):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.model = model\n",
    "\n",
    "        self.entity_anchor = nn.Parameter(torch.zeros((66, 768)))\n",
    "        torch.nn.init.uniform_(self.entity_anchor, a=-1.0, b=1.0)\n",
    "        \n",
    "        self.relation_embeddings = nn.Parameter(torch.zeros((57,3*768)))\n",
    "        torch.nn.init.uniform_(self.relation_embeddings, a=-1.0, b=1.0)            \n",
    "        self.nota_embeddings = nn.Parameter(torch.zeros((20,3*768)))\n",
    "        torch.nn.init.uniform_(self.nota_embeddings, a=-1.0, b=1.0)\n",
    "\n",
    "\n",
    "        self.triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "        self.at_loss = ATLoss()\n",
    "\n",
    "        self.k_mentions = 50\n",
    "                \n",
    "        self.cls_token_id = cls_token_id\n",
    "        self.sep_token_id = sep_token_id\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "        \n",
    "        with open(\"roles.json\") as f:\n",
    "            self.relation_types = json.load(f)\n",
    "        with open(\"trigger_entity_types.json\") as f:\n",
    "            self.types = json.load(f)\n",
    "        with open(\"feasible_roles.json\") as f:\n",
    "            self.feasible_roles = json.load(f)\n",
    "        \n",
    "    def encode(self, input_ids, attention_mask):\n",
    "        config = self.config\n",
    "        if type(config) == BertConfig or type(config) == DistilBertConfig:\n",
    "            start_tokens = [self.cls_token_id]\n",
    "            end_tokens = [self.sep_token_id]\n",
    "        elif type(config) == RobertaConfig or type(config) == XLMRobertaConfig:\n",
    "            start_tokens = [self.cls_token_id]\n",
    "            end_tokens = [self.sep_token_id, self.sep_token_id]\n",
    "        sequence_output, attention = process_long_input(self.model, input_ids, attention_mask, start_tokens, end_tokens)\n",
    "        return sequence_output, attention\n",
    "\n",
    "\n",
    "   \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, candidate_spans, relation_labels, entity_spans, entity_types, entity_ids):\n",
    "        sequence_output, attention = self.encode(input_ids, attention_mask)\n",
    "        loss = torch.zeros((1)).to(sequence_output)\n",
    "        mention_loss = torch.zeros((1)).to(sequence_output)\n",
    "        counter = 0\n",
    "        batch_triples = []\n",
    "        batch_events = []\n",
    "        batch_text = []\n",
    "        \n",
    "        for batch_i in range(sequence_output.size(0)):\n",
    "            text_i = self.tokenizer.convert_ids_to_tokens(input_ids[batch_i])\n",
    "            batch_text.append(text_i)\n",
    "\n",
    "            # MENTION DETECTION\n",
    "\n",
    "            # ---------- Candidate span embeddings ------------\n",
    "            mention_candidates = []\n",
    "            candidates_attentions = []\n",
    "            for span in candidate_spans[batch_i]:\n",
    "                mention_embedding = torch.mean(sequence_output[batch_i, span[0]:span[1]+1,:], 0)\n",
    "                mention_attention = torch.mean(attention[batch_i, span[0]:span[1]+1,:], 0)\n",
    "                #print(sequence_output[batch_i, span[0]:span[1],:].size())\n",
    "                mention_candidates.append(mention_embedding)\n",
    "                candidates_attentions.append(mention_attention)\n",
    "            embs = torch.stack(mention_candidates)\n",
    "            atts = torch.stack(candidates_attentions)\n",
    "\n",
    "            # ---------- Soft mention detection (scores) ------------\n",
    "            span_scores = embs.unsqueeze(1) * self.entity_anchor.unsqueeze(0)\n",
    "            span_scores = torch.sum(span_scores, dim=-1)\n",
    "            span_scores_max, class_for_span = torch.max(span_scores, dim=-1)\n",
    "            scores_for_max, max_spans = torch.topk(span_scores_max.view(-1), min(self.k_mentions, embs.size(0)), dim=0)\n",
    "            class_for_max_span = class_for_span[max_spans]\n",
    "\n",
    "            #types = ['TRIGGER','VAL', 'ABS', 'SID', 'MHI', 'BOD', 'VEH', 'GPE', 'MON', 'PER', 'TTL', 'LOC', 'WEA', 'INF', 'COM', 'FAC', 'CRM', 'ORG']\n",
    "            types = self.types\n",
    "            \n",
    "\n",
    "            if self.training:\n",
    "                # ---------- Mention Loss and adding true spans during training ------------\n",
    "\n",
    "                spans_for_type = {}\n",
    "\n",
    "                for span, rtype in zip(entity_spans[batch_i], entity_types[batch_i]):\n",
    "                    if rtype not in spans_for_type.keys():\n",
    "                        spans_for_type[rtype] = []\n",
    "                    spans_for_type[rtype].append(span[0])\n",
    "\n",
    "                anchors, positives, negatives = [], [], []\n",
    "\n",
    "                for rtype, positive_examples in spans_for_type.items():\n",
    "\n",
    "                    # add negative examples from entity spans\n",
    "                    for pos in positive_examples:\n",
    "                        for rtype2, negative_examples in spans_for_type.items():\n",
    "                            if rtype2 == rtype:\n",
    "                                continue\n",
    "                            for neg in negative_examples:\n",
    "                                anchors.append(self.entity_anchor[types.index(rtype),:])\n",
    "                                positives.append(torch.mean(sequence_output[batch_i, pos[0]:pos[1]+1,:], 0))\n",
    "                                negatives.append(torch.mean(sequence_output[batch_i, neg[0]:neg[1]+1,:], 0))\n",
    "\n",
    "                    # add negative examples from candidate spans\n",
    "                    for pos in positive_examples:\n",
    "                        for neg in [x for x in candidate_spans[batch_i] if x not in entity_spans[batch_i]]:\n",
    "                            anchors.append(self.entity_anchor[types.index(rtype),:])\n",
    "                            positives.append(torch.mean(sequence_output[batch_i, pos[0]:pos[1]+1,:], 0))\n",
    "                            negatives.append(torch.mean(sequence_output[batch_i, neg[0]:neg[1]+1,:], 0))\n",
    "\n",
    "\n",
    "                mention_loss += self.triplet_loss(torch.stack(anchors), torch.stack(positives), torch.stack(negatives))\n",
    "\n",
    "\n",
    "            # ARGUMENT ROLE LABELING\n",
    "\n",
    "\n",
    "            if self.training:\n",
    "                # ---------- Pooling Entity Embeddings and Attentions ------------\n",
    "                entity_embeddings = []\n",
    "                entity_attentions = []\n",
    "                for ent in entity_spans[batch_i]:\n",
    "                    ent_embedding = torch.mean(sequence_output[batch_i, ent[0][0]:ent[0][1],:],0)\n",
    "                    entity_embeddings.append(ent_embedding)\n",
    "                    ent_attention = torch.mean(attention[batch_i,:,ent[0][0]:ent[0][1],:],1)\n",
    "                    entity_attentions.append(ent_attention)\n",
    "                if(len(entity_embeddings) == 0):\n",
    "                    continue\n",
    "                entity_embeddings = torch.stack(entity_embeddings)\n",
    "                entity_attentions = torch.stack(entity_attentions)\n",
    "            else:\n",
    "                entity_embeddings = embs[max_spans]\n",
    "                entity_attentions = atts[max_spans]\n",
    "                \n",
    "                \n",
    "\n",
    "                '''\n",
    "                Wie mache ich das, dass beim training die entity_types,entity_spans,relation_labels mit übergeben werden aber bei der inferenz nicht?\n",
    "                \n",
    "                entity_types = []\n",
    "                for c in class_for_max_span:\n",
    "                    entity_types.append(types[c])\n",
    "                    \n",
    "                entity_spans = []\n",
    "                for s in max_spans:\n",
    "                    entity_spans.append(candidate_spans[batch_i][s])\n",
    "                \n",
    "                    \n",
    "                    \n",
    "                '''\n",
    "                \n",
    "            # ---------- Localized Context Pooling ------------\n",
    "            relation_candidates = []\n",
    "            localized_context = []\n",
    "            concat_embs = []\n",
    "            triggers = []\n",
    "            for s in range(entity_embeddings.shape[0]):\n",
    "                if entity_types[batch_i][s].split(\".\")[-1] != \"TRIGGER\":\n",
    "                    continue\n",
    "                triggers.append(s)\n",
    "                for o in range(entity_embeddings.shape[0]):\n",
    "                    if s != o:\n",
    "\n",
    "                        relation_candidates.append((s,o))\n",
    "\n",
    "                        A_s = entity_attentions[s,:,:]\n",
    "                        A_o = entity_attentions[o,:,:]\n",
    "                        A = torch.mul(A_o,A_s)\n",
    "                        q = torch.sum(A,0)\n",
    "                        a = q / q.sum()\n",
    "                        H_T = sequence_output[batch_i].T\n",
    "                        c = torch.matmul(H_T,a)\n",
    "                        localized_context.append(c)\n",
    "\n",
    "                        concat_emb = torch.cat((entity_embeddings[s],entity_embeddings[o],c),0)\n",
    "                        concat_embs.append(concat_emb)\n",
    "            if(len(localized_context) == 0):\n",
    "                continue\n",
    "            localized_context = torch.stack(localized_context)\n",
    "            embs = torch.stack(concat_embs)\n",
    "            \n",
    "            triggers = list(set(triggers))\n",
    "            # ---------- Pairwise Comparisons and Predictions ------------\n",
    "\n",
    "            scores = torch.matmul(embs,self.relation_embeddings.T)\n",
    "            nota_scores = torch.matmul(embs,self.nota_embeddings.T)\n",
    "            nota_scores = nota_scores.max(dim=-1,keepdim=True)[0]\n",
    "            scores = torch.cat((nota_scores, scores), dim=-1)\n",
    "            predictions = torch.argmax(scores, dim=-1, keepdim=False)\n",
    "            #Achtung: NOTA wird an 0. Stelle gesetzt\n",
    "            \n",
    "            if self.training:\n",
    "            # ---------- ATLoss with one-hot encoding for true labels ------------\n",
    "                targets = []\n",
    "                for r in relation_candidates:\n",
    "                    onehot = torch.zeros(len(relation_types))\n",
    "                    if r in relation_labels[batch_i]:\n",
    "                        onehot[relation_labels[batch_i][r]] = 1.0\n",
    "                    targets.append(onehot)\n",
    "                targets = torch.stack(targets).to(self.model.device)\n",
    "                loss += self.at_loss(scores,targets)\n",
    "                counter += 1\n",
    "            \n",
    "            # ---------- Inference ------------\n",
    "            triples = []\n",
    "            for idx,pair in enumerate(relation_candidates):\n",
    "                triple = {\n",
    "                    pair:relation_types[predictions[idx]]\n",
    "                }\n",
    "                triples.append(triple)\n",
    "            batch_triples.append(triples)\n",
    "                \n",
    "            events = []\n",
    "            for trigger in triggers:\n",
    "                trigger_word = entity_types[batch_i][trigger]\n",
    "                event_type = trigger_word.split(\".TRIGGER\")[0]\n",
    "                trigger_span = entity_spans[batch_i][trigger][0]\n",
    "                t_start = trigger_span[0]\n",
    "                t_end = trigger_span[1]\n",
    "                arguments = []\n",
    "                #print(f\"event_type: {event_type}, feasible roles: {self.feasible_roles[event_type]}\")\n",
    "                for t in triples:\n",
    "                    #Triples ist Liste von dicts, gibt es eine geschicktere Möglichkeit an s,o,r zu kommen?\n",
    "                    for i in t.items():\n",
    "                        s = i[0][0]\n",
    "                        o = i[0][1]\n",
    "                        r = i[1] \n",
    "                        \n",
    "                        if s == trigger:\n",
    "                            if r in self.feasible_roles[event_type]:\n",
    "                                a_span = entity_spans[batch_i][o][0]\n",
    "                                a_start = a_span[0]\n",
    "                                a_end = a_span[1]\n",
    "                                argument = {\n",
    "                                    \n",
    "                                    'role':r,\n",
    "                                    'start':a_start,\n",
    "                                    'end':a_end,\n",
    "                                    'text':batch_text[batch_i][a_start:a_end][0]\n",
    "                                }\n",
    "                                arguments.append(argument)\n",
    "                \n",
    "                event = {\n",
    "                    'event_type':event_type,\n",
    "                    'trigger': {'start':t_start ,'end':t_end, 'text':batch_text[batch_i][t_start:t_end][0]},\n",
    "                    'arguments':arguments\n",
    "                }\n",
    "                events.append(event)\n",
    "            batch_events.append(events)\n",
    "        if(counter == 0):\n",
    "                return torch.autograd.Variable(loss,requires_grad=True)\n",
    "        else:\n",
    "            return (mention_loss+loss)/counter, batch_triples, batch_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1851da41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexrothmaier/Documents/docred_dataloader_alex/.venv/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mymodel = Encoder(lm_config,lm_model, cls_token_id=tokenizer.convert_tokens_to_ids(tokenizer.cls_token), \n",
    "                                            sep_token_id=tokenizer.convert_tokens_to_ids(tokenizer.sep_token))\n",
    "mymodel.train()\n",
    "optimizer = AdamW(mymodel.parameters(), lr=1e-5, eps=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff6eb37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 103/103 [00:00<00:00, 4475.34it/s]\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "with tqdm.tqdm(dev_loader) as progress_bar:\n",
    "    for sample in progress_bar:\n",
    "\n",
    "        token_ids, input_mask, entity_spans, entity_types, entity_ids, relation_labels, text, token_map, candidate_spans = sample\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b1edf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss ,triples, events = mymodel(token_ids, input_mask, candidate_spans, relation_labels, entity_spans, entity_types, entity_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e192632c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scenario_en_kairos_35-E2',\n",
       " 'scenario_en_kairos_35-T15',\n",
       " 'scenario_en_kairos_35-T16',\n",
       " 'scenario_en_kairos_35-E3',\n",
       " 'scenario_en_kairos_35-T18',\n",
       " 'scenario_en_kairos_35-E4',\n",
       " 'scenario_en_kairos_35-T21',\n",
       " 'scenario_en_kairos_35-E5',\n",
       " 'scenario_en_kairos_35-T26',\n",
       " 'scenario_en_kairos_35-T28',\n",
       " 'scenario_en_kairos_35-T31',\n",
       " 'scenario_en_kairos_35-E6',\n",
       " 'scenario_en_kairos_35-T21',\n",
       " 'scenario_en_kairos_35-E7',\n",
       " 'scenario_en_kairos_35-T34',\n",
       " 'scenario_en_kairos_35-T35',\n",
       " 'scenario_en_kairos_35-T39',\n",
       " 'scenario_en_kairos_35-E1',\n",
       " 'scenario_en_kairos_35-T41',\n",
       " 'scenario_en_kairos_35-E8',\n",
       " 'scenario_en_kairos_35-T42']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55f26dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Conflict.Attack.Unspecified.TRIGGER',\n",
       " 'GPE',\n",
       " 'WEA',\n",
       " 'Life.Die.Unspecified.TRIGGER',\n",
       " 'PER',\n",
       " 'Justice.ArrestJailDetain.Unspecified.TRIGGER',\n",
       " 'PER',\n",
       " 'Justice.ArrestJailDetain.Unspecified.TRIGGER',\n",
       " 'PER',\n",
       " 'FAC',\n",
       " 'PER',\n",
       " 'Justice.ReleaseParole.Unspecified.TRIGGER',\n",
       " 'PER',\n",
       " 'Justice.ArrestJailDetain.Unspecified.TRIGGER',\n",
       " 'PER',\n",
       " 'FAC',\n",
       " 'PER',\n",
       " 'Justice.ReleaseParole.Unspecified.TRIGGER',\n",
       " 'PER',\n",
       " 'Justice.ArrestJailDetain.Unspecified.TRIGGER',\n",
       " 'PER']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_types[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad83d7",
   "metadata": {},
   "source": [
    "## Sanity Check\n",
    "\n",
    "--> offsets stimmen nicht  \n",
    "\n",
    "--> Nur die head tokens von den texts werden ausgebeben, die anderen subtokens fehlen\n",
    "\n",
    "(Arguments sind noch random, da relation_anchors random initialisiert sind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "6783c182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event_type': 'Conflict.Attack.DetonateExplode',\n",
       "  'trigger': {'start': 59, 'end': 62, 'text': 'de'},\n",
       "  'arguments': [{'role': 'Investigator',\n",
       "    'start': 146,\n",
       "    'end': 147,\n",
       "    'text': 'bombing'}]},\n",
       " {'event_type': 'Conflict.Attack.Unspecified',\n",
       "  'trigger': {'start': 155, 'end': 163, 'text': 'the'},\n",
       "  'arguments': [{'role': 'Investigator', 'start': 59, 'end': 62, 'text': 'de'},\n",
       "   {'role': 'Investigator', 'start': 146, 'end': 147, 'text': 'bombing'}]},\n",
       " {'event_type': 'Conflict.Attack.DetonateExplode',\n",
       "  'trigger': {'start': 146, 'end': 147, 'text': 'bombing'},\n",
       "  'arguments': [{'role': 'TeacherTrainer',\n",
       "    'start': 59,\n",
       "    'end': 62,\n",
       "    'text': 'de'},\n",
       "   {'role': 'Instrument', 'start': 155, 'end': 163, 'text': 'the'}]}]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "13e05869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'scenario_en_kairos_13-E1',\n",
       "  'event_type': 'Conflict.Attack.DetonateExplode',\n",
       "  'trigger': {'start': 46, 'end': 47, 'text': 'detonation', 'sent_idx': 3},\n",
       "  'arguments': []},\n",
       " {'id': 'scenario_en_kairos_13-E2',\n",
       "  'event_type': 'Conflict.Attack.Unspecified',\n",
       "  'trigger': {'start': 136,\n",
       "   'end': 144,\n",
       "   'text': 'the attacks of September 11, 2001.',\n",
       "   'sent_idx': 6},\n",
       "  'arguments': []},\n",
       " {'id': 'scenario_en_kairos_13-E3',\n",
       "  'event_type': 'Conflict.Attack.DetonateExplode',\n",
       "  'trigger': {'start': 127, 'end': 128, 'text': 'bombing', 'sent_idx': 6},\n",
       "  'arguments': []}]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "input_file = \"data/WikiEvents/train.jsonl\"\n",
    "with open(input_file) as f:\n",
    "    lines = f.read().splitlines()\n",
    "    df_inter = pd.DataFrame(lines)\n",
    "    df_inter.columns = ['json_element']\n",
    "    df_inter['json_element'].apply(json.loads)\n",
    "    df_we = pd.json_normalize(df_inter['json_element'].apply(json.loads))\n",
    "df_we.event_mentions[204]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0701c25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
